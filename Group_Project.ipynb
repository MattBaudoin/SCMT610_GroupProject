{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattBaudoin/SCMT610_GroupProject/blob/dev/Group_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSbqqk9EMTT5"
      },
      "source": [
        "Group 1\n",
        "\n",
        "* **Matthew Baudoin** -\n",
        "* **Kevin Brown** -\n",
        "* **Chelsea Jacobo** -\n",
        "* **Grace Morris** -\n",
        "* **Stephie Noel** -\n",
        "* **Kal Zapotocky** -\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOTE: Our Kaggle dataset did not come with any source code, so everything was written from scratch."
      ],
      "metadata": {
        "id": "uC75-7velnxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Environment Setup: If you are in Google Colab, run steps 1-3 and then skip to step 5**\n"
      ],
      "metadata": {
        "id": "yViP2mikJ_21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Environment Setup: If you are on a local IDE, run steps 2, 4, 5.**"
      ],
      "metadata": {
        "id": "5jDHrgOZpEt1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOnZKCsrMxOe",
        "outputId": "5fc1ecaf-1495-4d0f-95fe-7d378be8f155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "#1 - Environment setup: Connect Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 - Environment setup: load dependencies & NLP Setup\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mtick\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import unicodedata\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def ensure_nltk_resource(resource):\n",
        "    try:\n",
        "        nltk.data.find(resource)\n",
        "    except LookupError:\n",
        "        nltk.download(resource.split(\"/\")[-1])"
      ],
      "metadata": {
        "id": "TIxDvLJMyTLY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BiKv5rYAWHh3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d3bee8-f570-406f-8c4b-c54664540445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration validated\n",
            "RAW_REVIEW_PATH: /content/drive/Shared drives/Team1Share/GroupProject/Yelp_Dataset/yelp_academic_dataset_review.json\n"
          ]
        }
      ],
      "source": [
        "#3 - Configuration: Paths: In Colab\n",
        "\n",
        "BASE_DIR = Path(\"/content/drive/Shared drives/Team1Share/GroupProject/\")\n",
        "\n",
        "RAW_DIR = BASE_DIR / \"Yelp_Dataset\"\n",
        "RAW_REVIEW_FILE = \"yelp_academic_dataset_review.json\"\n",
        "RAW_REVIEW_PATH = RAW_DIR / RAW_REVIEW_FILE\n",
        "\n",
        "SLIM_PARQUET_DIR = BASE_DIR / \"reviews_slim_parquet\"\n",
        "TEXT_CLEAN_CHUNK_DIR = BASE_DIR / \"reviews_text_clean_parquet_chunks\"\n",
        "\n",
        "# validation and folder creation\n",
        "assert RAW_REVIEW_PATH.exists(), f\"Dataset not found: {RAW_REVIEW_PATH}\"\n",
        "\n",
        "SLIM_PARQUET_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TEXT_CLEAN_CHUNK_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Configuration validated\")\n",
        "print(\"RAW_REVIEW_PATH:\", RAW_REVIEW_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run step 4, only if you are working in an IDE with dataset on local machine.**"
      ],
      "metadata": {
        "id": "o0vb95QqoawR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4 - Configuration: Paths: On Desktop\n",
        "\n",
        "# This works as long as your project file is in the same folder as your dataset folders.\n",
        "\n",
        "BASE_DIR = Path.cwd()\n",
        "\n",
        "RAW_DIR = BASE_DIR / \"Yelp_Dataset\"\n",
        "RAW_REVIEW_FILE = \"yelp_academic_dataset_review.json\"\n",
        "RAW_REVIEW_PATH = RAW_DIR / RAW_REVIEW_FILE\n",
        "\n",
        "SLIM_PARQUET_DIR = BASE_DIR / \"reviews_slim\"\n",
        "TEXT_CLEAN_CHUNK_DIR = BASE_DIR / \"reviews_text_clean\"\n",
        "\n",
        "# validation and folder creation\n",
        "assert RAW_REVIEW_PATH.exists(), f\"Dataset not found: {RAW_REVIEW_PATH}\"\n",
        "\n",
        "SLIM_PARQUET_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TEXT_CLEAN_CHUNK_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Configuration validated\")\n",
        "print(\"RAW_REVIEW_PATH:\", RAW_REVIEW_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "SCSGavsrEesI",
        "outputId": "476b6d72-b7b4-4b8a-cb4d-825f57d48b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Dataset not found: /content/Yelp_Dataset/yelp_academic_dataset_review.json",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1690134802.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# validation and folder creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mRAW_REVIEW_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Dataset not found: {RAW_REVIEW_PATH}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mSLIM_PARQUET_DIR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Dataset not found: /content/Yelp_Dataset/yelp_academic_dataset_review.json"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 - NLTK setup\n",
        "\n",
        "# NTLK resources + stopwords/lemmatizer\n",
        "\n",
        "ensure_nltk_resource(\"corpora/stopwords\")\n",
        "ensure_nltk_resource(\"corpora/wordnet\")\n",
        "ensure_nltk_resource(\"corpora/omw-1.4\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Keep negations (critical for sentiment / low-satisfaction language)\n",
        "NEGATIONS = {\"no\", \"not\", \"nor\", \"never\", \"n't\"}\n",
        "stop_words = stop_words - NEGATIONS\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(f\"Stopwords loaded: {len(stop_words)} (negations kept)\")"
      ],
      "metadata": {
        "id": "SYdyPyfHW8il",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af1b595-b548-4a5e-9db9-8570e47dccad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopwords loaded: 195 (negations kept)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Blocks 6-8 dont need to be run again. This was only to inspect the original database**"
      ],
      "metadata": {
        "id": "MwcVMRJOL1q_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFetOPIsZt74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "5b3a9dc1-cef8-4616-bd67-2561abfae501"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "File /content/Yelp_Dataset/yelp_academic_dataset_review.json does not exist",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-726218782.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mSAMPLE_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10_000\u001b[0m  \u001b[0;31m# number of lines to sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAW_REVIEW_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSAMPLE_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Quick inspection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mconvert_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     json_reader = JsonReader(\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ujson\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data_from_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         ):\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File {filepath_or_buffer} does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: File /content/Yelp_Dataset/yelp_academic_dataset_review.json does not exist"
          ]
        }
      ],
      "source": [
        "# 6 - Exploratory Data Analysis: Sample the raw JSONL and inspect\n",
        "# NOTE: Do NOT load the full ~5GB JSONL into memory in Colab.\n",
        "\n",
        "# The following code is retained for reference, but is not executed.\n",
        "\n",
        "# df_raw = pd.read_json(DATA_PATH, lines=True)\n",
        "\n",
        "# Instead, Load a small sample of the JSON Lines file for exploratory analysis\n",
        "\n",
        "SAMPLE_N = 10_000  # number of lines to sample\n",
        "\n",
        "df_sample = pd.read_json(RAW_REVIEW_PATH, lines=True, nrows=SAMPLE_N)\n",
        "\n",
        "# Quick inspection\n",
        "display(df_sample.head())\n",
        "print(\"\\nShape:\", df_sample.shape)\n",
        "print(\"\\nColumns:\", df_sample.columns.tolist())\n",
        "\n",
        "# Check missing values (top 15)\n",
        "print(\"\\nMissing values (top 15):\")\n",
        "print(df_sample.isna().sum().sort_values(ascending=False).head(15))\n",
        "\n",
        "# Basic stats for star ratings if present\n",
        "if \"stars\" in df_sample.columns:\n",
        "    print(\"\\nStars distribution:\")\n",
        "    print(df_sample[\"stars\"].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AMeDXrRpZA8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "c7add4e5-6427-4d9c-b09f-c04488a52914"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_sample' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-554486450.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 7 - Quick text sanity checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_len_chars\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_len_words\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_sample' is not defined"
          ]
        }
      ],
      "source": [
        "# 7 - Quick text sanity checks\n",
        "\n",
        "df_sample[\"text_len_chars\"] = df_sample[\"text\"].str.len()\n",
        "df_sample[\"text_len_words\"] = df_sample[\"text\"].str.split().str.len()\n",
        "\n",
        "print(\"Avg chars:\", df_sample[\"text_len_chars\"].mean())\n",
        "print(\"Avg words:\", df_sample[\"text_len_words\"].mean())\n",
        "print(\"\\nWord length percentiles:\")\n",
        "print(df_sample[\"text_len_words\"].quantile([0.5, 0.9, 0.95, 0.99]))\n",
        "\n",
        "print(\"\\nExamples of short reviews:\")\n",
        "display(df_sample.loc[df_sample[\"text_len_words\"] <= 5, [\"stars\", \"text\"]].head(10))\n",
        "\n",
        "print(\"\\nExamples of long reviews:\")\n",
        "display(df_sample.sort_values(\"text_len_words\", ascending=False)[[\"stars\",\"text\"]].head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nugdja8GYdrR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "186f7aee-4922-487e-9805-12a029235747"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_sample' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2351134564.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 6 - Dataset dimensions and schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_sample' is not defined"
          ]
        }
      ],
      "source": [
        "# 8 - Dataset dimensions and schema\n",
        "\n",
        "df_sample.info()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You only NEED to run step 9-14 assuming you intend to clean and reduce noise from the semi-raw slimmed dataset.\n",
        "\n",
        "# If this isnt you, skip to step 15"
      ],
      "metadata": {
        "id": "E0EriASlRpw3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyv2EgsWqA6t"
      },
      "outputs": [],
      "source": [
        "# 9 - One time Dataset construction retaining columns required for analysis: JSONL to Parquet\n",
        "# Useful columns are: review_id, business_id, stars, text, date\n",
        "#\n",
        "# NOTE: Tried using CSV for the slimmed dataset, however it is still too large to load into Colabs memory.\n",
        "# After research, parquet is the better file format, especially for large datasets for ML/Analytics pipelines\n",
        "\n",
        "\n",
        "existing_parts = glob.glob(os.path.join(SLIM_PARQUET_DIR, \"part_*.parquet\"))\n",
        "if existing_parts:\n",
        "    print(f\"Slim parquet already exists ({len(existing_parts)} files). Skipping rebuild.\")\n",
        "else:\n",
        "    os.makedirs(SLIM_PARQUET_DIR, exist_ok=True)\n",
        "\n",
        "# Record start time to measure end-to-end write performance\n",
        "    start_time = time.time()\n",
        "    n_parts = 0\n",
        "\n",
        "# Stream the large JSON Lines file in blocks to avoid memory issues\n",
        "# Each iteration writes one self-contained Parquet partition;\n",
        "# the full dataset is represented by the collection of part files\n",
        "    for df_block in pd.read_json(\n",
        "        RAW_REVIEW_PATH,\n",
        "        lines=True,\n",
        "        chunksize=100_000\n",
        "    ):\n",
        "        df_block = df_block[[\"review_id\", \"business_id\", \"stars\", \"text\", \"date\"]]\n",
        "\n",
        "    # Construct a unique filename for this block to create a partitioned dataset\n",
        "        part_path = os.path.join(SLIM_PARQUET_DIR, f\"part_{n_parts:05d}.parquet\")\n",
        "\n",
        "    # Write the current block to disk as a compressed Parquet file\n",
        "        df_block.to_parquet(\n",
        "            part_path,\n",
        "            compression=\"snappy\",\n",
        "            index=False,\n",
        "            engine=\"pyarrow\"\n",
        "        )\n",
        "\n",
        "    # Increment part counter for the next block\n",
        "        n_parts += 1\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"Reduced dataset written to: {SLIM_PARQUET_DIR}\")\n",
        "    print(f\"Parquet files written: {n_parts}\")\n",
        "    print(f\"Time elapsed: {elapsed / 60:.1f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kGa9CpwqXh_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "75b22e39-c6ed-49bc-f414-aab6b738c1bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parts found: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1462174660.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSLIM_PARQUET_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"part_*.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Parts found:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"First part:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# 10 - Post slimming sanity checks\n",
        "parts = sorted(glob.glob(os.path.join(SLIM_PARQUET_DIR, \"part_*.parquet\")))\n",
        "print(\"Parts found:\", len(parts))\n",
        "print(\"First part:\", parts[0])\n",
        "\n",
        "df_check = pd.read_parquet(parts[0], engine=\"pyarrow\")\n",
        "\n",
        "print(\"Shape:\", df_check.shape)\n",
        "print(\"Columns:\", df_check.columns.tolist())\n",
        "print(\"\\nDtypes:\\n\", df_check.dtypes)\n",
        "\n",
        "print(\"\\nNulls:\\n\", df_check.isna().sum())\n",
        "\n",
        "print(\"\\nStars distribution (first part):\")\n",
        "print(df_check[\"stars\"].value_counts().sort_index())\n",
        "\n",
        "print(\"\\nSample text:\")\n",
        "print(df_check.loc[0, \"text\"][:300])\n",
        "\n",
        "total_rows = 0\n",
        "for f in glob.glob(os.path.join(SLIM_PARQUET_DIR, \"part_*.parquet\")):\n",
        "    total_rows += pq.ParquetFile(f).metadata.num_rows\n",
        "print(\"Total rows written:\", total_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Conduct cleaning and noise reduction on the working dataset you loaded into memory**"
      ],
      "metadata": {
        "id": "IEYP0jzDS2DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#11 - Basic quality cleanup and text cleaning functions\n",
        "\n",
        "# Precompile regex once (faster)\n",
        "RE_URL_EMAIL = re.compile(r\"http\\S+|www\\.\\S+|\\S+@\\S+\")\n",
        "RE_NBSP_HTML = re.compile(r\"&nbsp;\")\n",
        "RE_WS = re.compile(r\"\\s+\")\n",
        "RE_ELONG = re.compile(r\"(.)\\1{2,}\")          # soooo -> soo\n",
        "RE_NUM = re.compile(r\"\\d+\")\n",
        "RE_PUNCT_KEEP_HYPHEN = re.compile(r\"[^\\w\\s\\-]\")  # remove punctuation but keep hyphen\n",
        "RE_APOSTROPHE = re.compile(r\"[’`´]\")         # normalize weird apostrophes to '\n",
        "\n",
        "# Contractions (simple, good-enough starter list)\n",
        "CONTRACTIONS = {\n",
        "    \"can't\": \"can not\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"n't\": \" not\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"i've\": \"i have\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"i'll\": \"i will\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"he's\": \"he is\",\n",
        "\n",
        "}\n",
        "# Build a regex that matches keys like \"can't\", \"won't\", etc.\n",
        "RE_CONTRACTIONS = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, CONTRACTIONS.keys())) + r\")\\b\")\n",
        "\n",
        "def expand_contractions(text: str) -> str:\n",
        "    # normalize apostrophes first\n",
        "    text = RE_APOSTROPHE.sub(\"'\", text)\n",
        "    return RE_CONTRACTIONS.sub(lambda m: CONTRACTIONS[m.group(0)], text)\n",
        "\n",
        "def reduce_character_noise(token: str) -> str:\n",
        "    return RE_ELONG.sub(r\"\\1\\1\", token)\n",
        "\n",
        "def clean_review_text_method(text: str) -> str:\n",
        "    # 1) Ensure string + Unicode normalize (keeps accents but normalizes representation)\n",
        "    text = \"\" if text is None else str(text)\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "\n",
        "    # 2) Normalize NBSP and whitespace-ish artifacts\n",
        "    text = text.replace(\"\\u00A0\", \" \")      # actual NBSP char\n",
        "    text = RE_NBSP_HTML.sub(\" \", text)      # literal &nbsp; if present\n",
        "\n",
        "    # 3) Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 4) Remove URLs/emails\n",
        "    text = RE_URL_EMAIL.sub(\" \", text)\n",
        "\n",
        "    # 5) Expand contractions BEFORE tokenization\n",
        "    text = expand_contractions(text)\n",
        "\n",
        "    # 6) Normalize numbers to a token\n",
        "    text = RE_NUM.sub(\" num_token \", text)\n",
        "\n",
        "    # 7) Punctuation policy:\n",
        "    #    - remove punctuation except hyphens\n",
        "    #    - then split hyphens to spaces (wait-time -> wait time)\n",
        "    text = RE_PUNCT_KEEP_HYPHEN.sub(\" \", text)\n",
        "    text = text.replace(\"-\", \" \")\n",
        "\n",
        "    # 8) Collapse whitespace\n",
        "    text = RE_WS.sub(\" \", text).strip()\n",
        "\n",
        "    # 9) Tokenize\n",
        "    tokens = text.split()\n",
        "\n",
        "    cleaned_tokens = []\n",
        "    for tok in tokens:\n",
        "        tok = reduce_character_noise(tok)\n",
        "\n",
        "        # keep num_token as-is\n",
        "        if tok == \"num_token\":\n",
        "            cleaned_tokens.append(tok)\n",
        "            continue\n",
        "\n",
        "        # keep alphabetic tokens that aren't stopwords (negations already preserved in stop_words)\n",
        "        if tok.isalpha() and tok not in stop_words:\n",
        "            tok = lemmatizer.lemmatize(tok)\n",
        "            cleaned_tokens.append(tok)\n",
        "\n",
        "    return \" \".join(cleaned_tokens)"
      ],
      "metadata": {
        "id": "q4mRrWUQdbcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12 - Apply cleaning method in manageable chunks\n",
        "# Output: one cleaned chunk per slim parquet part\n",
        "\n",
        "LOG_EVERY = 5          # log every N parts\n",
        "MIN_WORDS = 5          # your post-cleaning minimum (we can tune later)\n",
        "\n",
        "part_files = sorted(glob.glob(os.path.join(SLIM_PARQUET_DIR, \"part_*.parquet\")))\n",
        "total_parts = len(part_files)\n",
        "\n",
        "# Resume: detect already-written chunks\n",
        "existing_files = glob.glob(os.path.join(TEXT_CLEAN_CHUNK_DIR, \"chunk_*.parquet\"))\n",
        "print(f\"Total parts to process: {total_parts}\")\n",
        "print(f\"Chunks already on disk: {len(existing_files)}\")\n",
        "\n",
        "t0 = time.time()\n",
        "processed = 0\n",
        "skipped = 0\n",
        "\n",
        "for part_idx, part_path in enumerate(part_files):\n",
        "    chunk_path = os.path.join(TEXT_CLEAN_CHUNK_DIR, f\"chunk_{part_idx:05d}.parquet\")\n",
        "\n",
        "    # Resume: skip completed\n",
        "    if os.path.exists(chunk_path):\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    df = pd.read_parquet(part_path, engine=\"pyarrow\")\n",
        "\n",
        "    # Basic cleanup (per part)\n",
        "    df[\"text\"] = df[\"text\"].astype(\"string\")\n",
        "    df = df.dropna(subset=[\"text\"])\n",
        "    df = df[df[\"text\"].str.strip() != \"\"]\n",
        "    df = df.drop_duplicates(subset=[\"text\"]).reset_index(drop=True)  # within-part dedupe\n",
        "\n",
        "    # Apply cleaning\n",
        "    df[\"text_clean\"] = df[\"text\"].map(clean_review_text_method)\n",
        "\n",
        "    # Drop empty after cleaning\n",
        "    df = df[df[\"text_clean\"].str.strip() != \"\"]\n",
        "\n",
        "    # Min word filter (vectorized)\n",
        "    df[\"clean_word_count\"] = df[\"text_clean\"].str.split().str.len()\n",
        "    df = df[df[\"clean_word_count\"] >= MIN_WORDS]\n",
        "\n",
        "    # Keep only needed columns (drop raw text)\n",
        "    df = df[[\"review_id\", \"business_id\", \"stars\", \"date\", \"text_clean\"]].reset_index(drop=True)\n",
        "\n",
        "    # Write checkpoint\n",
        "    tmp_path = chunk_path + \".tmp\"\n",
        "    df.to_parquet(tmp_path, index=False, compression=\"snappy\", engine=\"pyarrow\")\n",
        "    os.replace(tmp_path, chunk_path)  # atomic on Windows\n",
        "\n",
        "    processed += 1\n",
        "\n",
        "    if processed % LOG_EVERY == 0:\n",
        "        elapsed = time.time() - t0\n",
        "        done = part_idx + 1\n",
        "        pct = 100 * done / total_parts\n",
        "        rate = done / elapsed if elapsed > 0 else 0\n",
        "        eta_min = (total_parts - done) / rate / 60 if rate > 0 else float(\"inf\")\n",
        "        print(f\"[{done:>3}/{total_parts}] {pct:>6.2f}% | elapsed {elapsed/60:>6.1f}m | ETA {eta_min:>6.1f}m | wrote {len(df):,} rows\")\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "print(f\"Cleaning complete. processed={processed}, skipped={skipped}, elapsed={elapsed/60:.1f} min\")\n",
        "print(\"Output:\", TEXT_CLEAN_CHUNK_DIR)"
      ],
      "metadata": {
        "id": "Q2Re7a1Pg-pK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fcaffaa-fe4a-4315-b624-553cd8f4d54f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parts to process: 0\n",
            "Chunks already on disk: 0\n",
            "Cleaning complete. processed=0, skipped=0, elapsed=0.0 min\n",
            "Output: /content/reviews_text_clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13 - Checking the file size of our cleaned database\n",
        "\n",
        "files = glob.glob(os.path.join(TEXT_CLEAN_CHUNK_DIR, \"chunk_*.parquet\"))\n",
        "print(\"Chunk files:\", len(files))\n",
        "\n",
        "if not files:\n",
        "    print(\"No chunk files found yet.\")\n",
        "else:\n",
        "    total_bytes = sum(os.path.getsize(f) for f in files)\n",
        "    print(\"Total size (GB):\", total_bytes / (1024**3))\n",
        "    print(\"Smallest file (MB):\", min(os.path.getsize(f) for f in files) / (1024**2))\n",
        "    print(\"Largest file (MB):\", max(os.path.getsize(f) for f in files) / (1024**2))"
      ],
      "metadata": {
        "id": "lXLYIpYbLa5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b28f7627-5c92-4991-823a-d3cbdc0bf257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk files: 0\n",
            "No chunk files found yet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 14 - Post-cleaning validation: confirm cleaned chunk outputs are complete and sane\n",
        "# - Counts cleaned chunk files and total disk usage\n",
        "# - Samples a few chunks to verify schema, nulls, stars distribution, text lengths\n",
        "# - Verifies key cleaning policies (num_token presence, negations retained)\n",
        "chunk_files = sorted(glob.glob(os.path.join(TEXT_CLEAN_CHUNK_DIR, \"chunk_*.parquet\")))\n",
        "print(\"Cleaned chunks found:\", len(chunk_files))\n",
        "assert len(chunk_files) > 0, \"No cleaned chunks found.\"\n",
        "\n",
        "# Load a few chunks (first, middle, last) for quick checks\n",
        "sample_idxs = [0, len(chunk_files)//2, len(chunk_files)-1]\n",
        "dfs = []\n",
        "for idx in sample_idxs:\n",
        "    df = pd.read_parquet(chunk_files[idx], engine=\"pyarrow\")\n",
        "    df[\"chunk_id\"] = idx\n",
        "    dfs.append(df)\n",
        "\n",
        "df_check = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "print(\"\\nColumns:\", df_check.columns.tolist())\n",
        "print(\"\\nDtypes:\\n\", df_check.dtypes)\n",
        "\n",
        "# Basic sanity\n",
        "print(\"\\nRows sampled:\", len(df_check))\n",
        "print(\"Nulls:\\n\", df_check.isna().sum())\n",
        "\n",
        "# Stars distribution (in sampled chunks)\n",
        "print(\"\\nStars distribution (sampled chunks):\")\n",
        "print(df_check[\"stars\"].value_counts().sort_index())\n",
        "\n",
        "# Text length distribution\n",
        "df_check[\"clean_words\"] = df_check[\"text_clean\"].str.split().str.len()\n",
        "print(\"\\nClean word count percentiles:\")\n",
        "print(df_check[\"clean_words\"].quantile([0.1, 0.5, 0.9, 0.95, 0.99]))\n",
        "\n",
        "# Verify policy effects\n",
        "print(\"\\nnum_token frequency (docs containing it):\",\n",
        "      (df_check[\"text_clean\"].str.contains(r\"\\bnum_token\\b\")).mean())\n",
        "\n",
        "negation_terms = [\"not\", \"no\", \"never\"]\n",
        "for term in negation_terms:\n",
        "    frac = (df_check[\"text_clean\"].str.contains(rf\"\\b{term}\\b\")).mean()\n",
        "    print(f\"Docs containing '{term}': {frac:.3f}\")\n",
        "\n",
        "# Quick samples\n",
        "print(\"\\nSample cleaned texts:\")\n",
        "display(df_check[[\"stars\",\"text_clean\"]].head(10))"
      ],
      "metadata": {
        "id": "DnqhF5LslX9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "307ec171-ce05-4f5a-8181-505a77cb20a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned chunks found: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "No cleaned chunks found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1732939129.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mchunk_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT_CLEAN_CHUNK_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chunk_*.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cleaned chunks found:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No cleaned chunks found.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load a few chunks (first, middle, last) for quick checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: No cleaned chunks found."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run step 15 next, it defines utilities and helpers that will assist with analyzing the cleaned dataset**"
      ],
      "metadata": {
        "id": "XSt-1nPLKQGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 - Define helpers and utilities\n",
        "\n",
        "def iter_parquet_chunks(parquet_dir, pattern=\"chunk_*.parquet\", columns=None, engine=\"pyarrow\"):\n",
        "    parquet_dir = str(parquet_dir)  # allow Path objects\n",
        "    files = sorted(glob.glob(str(Path(parquet_dir) / pattern)))\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No parquet files found in {parquet_dir} matching {pattern}\")\n",
        "\n",
        "    for f in files:\n",
        "        yield f, pd.read_parquet(f, columns=columns, engine=engine)"
      ],
      "metadata": {
        "id": "Lj6XxRrlJea_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From here on we will be performing dataset analysis. As the dataset remains large (~1.6GB), we will use the above utility to stream the dataset from TEXT_CLEAN_CHUNK_DIR through any analysis steps.**"
      ],
      "metadata": {
        "id": "7d9VSCmusWsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 16 - Count the distribution of star ratings\n",
        "\n",
        "star_counts = Counter()\n",
        "\n",
        "for _, df_part in iter_parquet_chunks(\n",
        "    TEXT_CLEAN_CHUNK_DIR,\n",
        "    columns=[\"stars\"]\n",
        "):\n",
        "    star_counts += Counter(df_part[\"stars\"].value_counts().to_dict())\n",
        "\n",
        "print(star_counts)"
      ],
      "metadata": {
        "id": "SPF9BJWusVoO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "fbe7eb53-0de8-40da-fbf0-bb4c9152d723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "No parquet files found in /content/reviews_text_clean matching chunk_*.parquet",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4142741423.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstar_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m for _, df_part in iter_parquet_chunks(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mTEXT_CLEAN_CHUNK_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stars\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2749039948.py\u001b[0m in \u001b[0;36miter_parquet_chunks\u001b[0;34m(parquet_dir, pattern, columns, engine)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No parquet files found in {parquet_dir} matching {pattern}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No parquet files found in /content/reviews_text_clean matching chunk_*.parquet"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N-TW9zaOHrH"
      },
      "outputs": [],
      "source": [
        "# 17 - Show star rating distribution as a graph\n",
        "\n",
        "star_series = (\n",
        "    pd.Series(star_counts)\n",
        "    .sort_index()\n",
        ")\n",
        "\n",
        "star_series.plot(kind=\"bar\")\n",
        "plt.title(\"Distribution of Yelp Review Ratings\")\n",
        "plt.xlabel(\"Star Rating\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "\n",
        "# Format y-axis with commas\n",
        "plt.gca().yaxis.set_major_formatter(\n",
        "    mtick.FuncFormatter(lambda x, _: f\"{int(x):,}\")\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtY6aOkNRCS9"
      },
      "source": [
        "### Insight 1: Yelp Reviews Are Positively Skewed\n",
        "\n",
        "The distribution of Yelp star ratings shows a strong skew toward higher ratings,\n",
        "with the majority of reviews being 4 or 5 stars. This suggests that Yelp users\n",
        "are more likely to leave reviews after positive experiences, which may introduce\n",
        "a positivity bias in sentiment-based analyses. You can also see that there is more 1 star reviews than 2 & 3. This shows that people are likely more motivated to leave a review if they have an exceptionally good experience (4/5 stars) or an exceptionally poor experience (1 star)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC-37K-7RUwj"
      },
      "outputs": [],
      "source": [
        "# 18 - Count review lengths\n",
        "\n",
        "cap = 300\n",
        "bins = 50\n",
        "bin_edges = np.linspace(0, cap, bins + 1)   # 51 edges define 50 bins\n",
        "hist_counts = np.zeros(bins, dtype=np.int64)\n",
        "\n",
        "total_reviews = 0\n",
        "\n",
        "for _, df_part in iter_parquet_chunks(\n",
        "    TEXT_CLEAN_CHUNK_DIR,\n",
        "    columns=[\"text_clean\"]\n",
        "):\n",
        "    # word counts for this chunk\n",
        "    lengths = df_part[\"text_clean\"].str.split().str.len().astype(np.int32)\n",
        "\n",
        "    # cap long reviews\n",
        "    lengths = lengths.clip(upper=cap).to_numpy()\n",
        "\n",
        "    # update histogram counts\n",
        "    counts, _ = np.histogram(lengths, bins=bin_edges)\n",
        "    hist_counts += counts\n",
        "    total_reviews += len(lengths)\n",
        "\n",
        "print(\"Total reviews processed:\", total_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 19 - Summarize review lengths\n",
        "\n",
        "cum = np.cumsum(hist_counts)\n",
        "n = cum[-1]\n",
        "\n",
        "def approx_percentile(p):\n",
        "    target = p * n\n",
        "    idx = np.searchsorted(cum, target)\n",
        "    idx = min(idx, len(hist_counts) - 1)\n",
        "    return float((bin_edges[idx] + bin_edges[idx + 1]) / 2)\n",
        "\n",
        "summary = {\n",
        "    \"Total reviews processed\": f\"{n:,}\",\n",
        "    \"Median review length (50th percentile)\": approx_percentile(0.50),\n",
        "    \"90th percentile review length\": approx_percentile(0.90),\n",
        "    \"95th percentile review length\": approx_percentile(0.95),\n",
        "    \"99th percentile review length\": approx_percentile(0.99),\n",
        "    f\"Reviews at max cap ({cap} words)\": int(hist_counts[-1]),\n",
        "}\n",
        "\n",
        "for k, v in summary.items():\n",
        "    print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "id": "yTUHIb1WOQwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20 - Visualize review length with a histogram\n",
        "\n",
        "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
        "bar_width = bin_edges[1] - bin_edges[0]\n",
        "\n",
        "plt.bar(bin_centers, hist_counts, width=bar_width, align=\"center\")\n",
        "\n",
        "plt.title(\"Distribution of Review Lengths\")\n",
        "plt.xlabel(f\"Number of Words in text_clean (capped at {cap})\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "\n",
        "plt.gca().yaxis.set_major_formatter(\n",
        "    mtick.FuncFormatter(lambda x, _: f\"{int(x):,}\")\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TgHHBZovMmuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKqM86iEVsHY"
      },
      "source": [
        "### Insight 2: Review Lengths Are Right-Skewed\n",
        "\n",
        "The distribution of review lengths shows a strong right skew, with most Yelp\n",
        "reviews containing fewer than 100 words. A smaller number of reviews are much\n",
        "longer, forming a long tail. This indicates that while most users leave brief\n",
        "feedback, a subset of users provide detailed reviews that may contain richer\n",
        "context and sentiment for text analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 21 - Analyze relationship between review length and star rating (histogram-based)\n",
        "\n",
        "MAX_WORDS = 300\n",
        "\n",
        "# counts_by_star[star][k] = how many reviews of this star have length k (capped)\n",
        "counts_by_star = {star: np.zeros(MAX_WORDS + 1, dtype=np.int64) for star in [1, 2, 3, 4, 5]}\n",
        "\n",
        "for _, df_part in iter_parquet_chunks(\n",
        "    TEXT_CLEAN_CHUNK_DIR,\n",
        "    columns=[\"stars\", \"text_clean\"]\n",
        "):\n",
        "    lengths = (\n",
        "        df_part[\"text_clean\"]\n",
        "        .str.split()\n",
        "        .str.len()\n",
        "        .clip(upper=MAX_WORDS)\n",
        "        .to_numpy()\n",
        "    )\n",
        "    stars = df_part[\"stars\"].to_numpy()\n",
        "\n",
        "    for star in [1, 2, 3, 4, 5]:\n",
        "        star_lengths = lengths[stars == star]\n",
        "        if star_lengths.size:\n",
        "            counts_by_star[star] += np.bincount(star_lengths, minlength=MAX_WORDS + 1)"
      ],
      "metadata": {
        "id": "5ZNJdeYRwUBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22 - Calculate median review length per star from histogram counts\n",
        "\n",
        "def median_from_counts(counts: np.ndarray) -> int:\n",
        "    cum = np.cumsum(counts)\n",
        "    n = cum[-1]\n",
        "    if n == 0:\n",
        "        return 0\n",
        "    target = (n - 1) // 2  # 0-index median position\n",
        "    return int(np.searchsorted(cum, target))\n",
        "\n",
        "star_levels = [1, 2, 3, 4, 5]\n",
        "median_lengths = [median_from_counts(counts_by_star[s]) for s in star_levels]\n",
        "\n",
        "print(dict(zip(star_levels, median_lengths)))"
      ],
      "metadata": {
        "id": "eCYqJQ-JwdjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23 Visualize Median Review Length by Star Rating\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(star_levels, median_lengths, marker=\"o\")\n",
        "plt.xlabel(\"Star Rating\")\n",
        "plt.ylabel(\"Median Review Length (words, capped at 300)\")\n",
        "plt.title(\"Median Review Length by Star Rating\")\n",
        "plt.xticks(star_levels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gm9xefhGweKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insight:\n",
        "\n",
        "Low rated reviews have a tendancy to be longer than higher star ratings, suggesting that dissatisfied customers are more likely to provided detailed written feedback."
      ],
      "metadata": {
        "id": "ddGQTgQYxXFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 24 - Star Rating Distribution and Class Balance\n",
        "# For predicting dissatisfaction, we should reframe star ratings as a binary\n",
        "# 1-2 Stars is Low satisfaction\n",
        "# 3-5 Stars is Not low satisfaction\n",
        "\n",
        "assert sum(star_counts.values()) > 1_000_000, \"star_counts looks too small—did you compute it on full data?\"\n",
        "\n",
        "low_satisfaction = star_counts.get(1, 0) + star_counts.get(2, 0)\n",
        "not_low_satisfaction = (\n",
        "    star_counts.get(3, 0)\n",
        "    + star_counts.get(4, 0)\n",
        "    + star_counts.get(5, 0)\n",
        ")\n",
        "\n",
        "total_reviews = low_satisfaction + not_low_satisfaction\n",
        "if total_reviews == 0:\n",
        "    raise ValueError(\"total_reviews is 0; star_counts is empty.\")\n",
        "\n",
        "print(f\"Low satisfaction (1–2 stars): {low_satisfaction:,} \"\n",
        "      f\"({100 * low_satisfaction / total_reviews:.1f}%)\")\n",
        "\n",
        "print(f\"Not low satisfaction (3–5 stars): {not_low_satisfaction:,} \"\n",
        "      f\"({100 * not_low_satisfaction / total_reviews:.1f}%)\")"
      ],
      "metadata": {
        "id": "dKVhTRd-5tPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insight\n",
        "\n",
        "After dividing star ratings into two classes, low satisfaction (1–2 stars) and not low satisfaction (3–5 stars), it is clear that low satisfaction reviews represent the minority class. While a future model could achieve high accuracy by predicting “not low satisfaction” for most reviews, accurately identifying dissatisfied customers will require focusing on language patterns that are specifically associated with negative reviews."
      ],
      "metadata": {
        "id": "bqi48_02-Nly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating TF Matrix by creating corpus, converting data, etc.**"
      ],
      "metadata": {
        "id": "xo-F3TqwZmG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 25 - Build sample corpus for TF / TF-IDF\n",
        "\n",
        "# ML algorithms require numerical input, not raw text.\n",
        "# The TF matrix converts each review into a vector of word counts, enabling:\n",
        "#   * Classification models (sentiment prediction)\n",
        "#   * Clustering algorithms (topic discovery)\n",
        "#   * Similarity computations (finding related reviews)\n",
        "\n",
        "MAX_DOCS  = 200_000   # cap to control RAM\n",
        "MAX_WORDS = 300       # truncate long reviews based on prior EDA\n",
        "\n",
        "corpus = []          # list of cleaned review texts (documents)\n",
        "star_ratings = []    # corresponding star ratings\n",
        "\n",
        "for _, df_part in iter_parquet_chunks(TEXT_CLEAN_CHUNK_DIR, columns=[\"stars\", \"text_clean\"]):\n",
        "    remaining = MAX_DOCS - len(corpus)\n",
        "    if remaining <= 0:\n",
        "        break\n",
        "\n",
        "    df_part = df_part.head(remaining)\n",
        "\n",
        "    texts = (\n",
        "        df_part[\"text_clean\"]\n",
        "        .str.split()\n",
        "        .str[:MAX_WORDS]\n",
        "        .str.join(\" \")\n",
        "    )\n",
        "\n",
        "    corpus.extend(texts.tolist())\n",
        "    star_ratings.extend(df_part[\"stars\"].tolist())\n",
        "\n",
        "print(f\"Sample corpus size: {len(corpus):,} reviews\")"
      ],
      "metadata": {
        "id": "qmphQ06uWRf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26 - Build Term Frequency (TF) matrix\n",
        "# Tokenization: Splits text into individual words (tokens)\n",
        "# Vocabulary building: Creates a dictionary of all unique words\n",
        "# Counting: For each document, counts occurrences of each vocabulary word\n",
        "# Initialize CountVectorizer to create the TF matrix\n",
        "\n",
        "tf_vectorizer = CountVectorizer(\n",
        "    max_features=50_000,  # limit vocabulary size for stability\n",
        "    min_df=10,     # ignore terms that appear in <10 docs\n",
        "    max_df=0.9    # ignore terms that appear in >90% of docs\n",
        ")\n",
        "\n",
        "# The output is stored as a sparse matrix because:\n",
        "#   * Most entries are zero (a review uses only ~50 of 24,232 vocabulary words)\n",
        "#   * Sparse format stores only non-zero values, saving ~99% memory\n",
        "#   * Scikit-learn algorithms are optimized for sparse input\n",
        "# Fit and transform the corpus to get the TF matrix\n",
        "tf_matrix = tf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"TF matrix shape:\", tf_matrix.shape)"
      ],
      "metadata": {
        "id": "PsoLWph2ZfdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96fddf1c"
      },
      "source": [
        "#27 - Build TF - IDF Matrix\n",
        "# LIMITATION OF RAW TF: Simple word counts treat all words equally, but some\n",
        "#   words are more informative than others. \"Delicious\" appearing in a food\n",
        "#   review is more meaningful than \"food\" which appears in most reviews.\n",
        "\n",
        "# Initialize TfidfVectorizer to create the TF-IDF matrix\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    vocabulary=tf_vectorizer.vocabulary_,\n",
        "     sublinear_tf=True\n",
        ")\n",
        "\n",
        "# Fit and transform the corpus to get the TF-IDF matrix\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vocabulary Inspection**"
      ],
      "metadata": {
        "id": "pMQc3tVHkneU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d384d5c"
      },
      "source": [
        "#28 - Attempting to get a view of the TF Matrix results\n",
        "# VOCABULARY INSPECTION: Viewing feature names reveals:\n",
        "#   * Tokenization behavior (how words are split and normalized)\n",
        "#   * Presence of unexpected tokens (numbers, artifacts)\n",
        "#   * Vocabulary size and composition\n",
        "\n",
        "# 27 - Inspect TF vocabulary + first document TF counts\n",
        "\n",
        "\n",
        "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
        "print(f\"Total TF features: {len(tf_feature_names)}\")\n",
        "\n",
        "# A) One review: top TF counts\n",
        "doc_idx = 0\n",
        "first_review_tf = tf_matrix[doc_idx]\n",
        "nonzero_indices = first_review_tf.nonzero()[1]\n",
        "\n",
        "pairs = [(tf_feature_names[i], int(first_review_tf[0, i])) for i in nonzero_indices]\n",
        "pairs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f\"\\nTop TF counts for review #{doc_idx}:\")\n",
        "for term, cnt in pairs[:25]:\n",
        "    print(f\"  {term}: {cnt}\")\n",
        "\n",
        "# B) Whole corpus: top TF terms\n",
        "term_counts = np.asarray(tf_matrix.sum(axis=0)).ravel()\n",
        "top_idx = term_counts.argsort()[::-1][:25]\n",
        "\n",
        "print(\"\\nTop 25 terms by total TF count in corpus:\")\n",
        "for i in top_idx:\n",
        "    print(f\"  {tf_feature_names[i]}: {int(term_counts[i]):,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82e58ec6"
      },
      "source": [
        "# 29 - Inspect TF-IDF weights for the same review\n",
        "\n",
        "doc_idx = 0  # keep consistent with Cell 27\n",
        "\n",
        "first_review_tfidf = tfidf_matrix[doc_idx]\n",
        "nonzero_indices = first_review_tfidf.nonzero()[1]\n",
        "\n",
        "# Sort by highest TF-IDF weight\n",
        "pairs = [(tf_feature_names[i], float(first_review_tfidf[0, i])) for i in nonzero_indices]\n",
        "pairs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f\"Top TF-IDF weights for review #{doc_idx}:\")\n",
        "for term, w in pairs[:25]:\n",
        "    print(f\"  {term}: {w:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference between a TF Matrix and TF-IDF Matrix:\n",
        "\n",
        "\n",
        "*   A TF Matrix counts how many times each word appears in each document\n",
        "*   A TF-IDF Matrix weights each word by how distinctive it is across the entire corpus\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6RD88v0llw5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "term = \"opt\"\n",
        "idx = tf_vectorizer.vocabulary_.get(term)\n",
        "if idx is None:\n",
        "    print(\"Term not in vocabulary:\", term)\n",
        "else:\n",
        "    df_count = (tf_matrix[:, idx] > 0).sum()\n",
        "    total_docs = tf_matrix.shape[0]\n",
        "    print(f\"Docs containing '{term}': {int(df_count):,} / {total_docs:,} ({100*df_count/total_docs:.3f}%)\")"
      ],
      "metadata": {
        "id": "_vRT2aVAmMOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term = \"opt\"\n",
        "examples = []\n",
        "\n",
        "for i, txt in enumerate(corpus):\n",
        "    if \" opt \" in f\" {txt} \":\n",
        "        examples.append((i, star_ratings[i], txt))\n",
        "    if len(examples) >= 10:\n",
        "        break\n",
        "\n",
        "for i, stars, txt in examples:\n",
        "    print(f\"\\nDoc #{i} | stars={stars}\\n{txt[:500]}\")"
      ],
      "metadata": {
        "id": "EFzxpBq_mMgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GaREAxPTTo9O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}