{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgU9mIylnxcnmMXZrjL+AR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattBaudoin/SCMT610_GroupProject/blob/main/Group_Project_Part_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group 1\n",
        "\n",
        "Matthew Baudoin\n",
        "Kevin Brown\n",
        "Chelsea Jacobo\n",
        "Grace Morris\n",
        "Stephie Noel\n",
        "Kal Zapotocky"
      ],
      "metadata": {
        "id": "rSbqqk9EMTT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment setup: Mount Google Drive and load dependencies\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "wOnZKCsrMxOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset configuration: define path and verify accessibility\n",
        "\n",
        "BASE = '/content/drive/Shared drives/Team1Share/GroupProject/Yelp_Dataset'\n",
        "FILE = 'yelp_academic_dataset_review.json'\n",
        "DATA_PATH = os.path.join(BASE, FILE)\n",
        "\n",
        "assert os.path.exists(DATA_PATH), f\"Dataset not found: {DATA_PATH}\"\n",
        "print(\"✅ Dataset path verified\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiKv5rYAWHh3",
        "outputId": "c874c5e6-d491-4c8c-9a79-9e857517af22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset path verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from JSON Lines file\n",
        "# NOTE: Attempting to load the full dataset (~5GB) will crash the colab runtime environment.\n",
        "# The following code is retained for reference, but is not exectuted.\n",
        "\n",
        "# df_raw = pd.read_json(DATA_PATH, lines=True)\n",
        "\n",
        "# Instead, Load a small sample of the JSON Lines file for exploratory analysis\n",
        "rows = []\n",
        "with open(DATA_PATH, \"r\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        rows.append(json.loads(line))\n",
        "        if i >= 10_000:   # sample size for exploratory inspection\n",
        "            break\n",
        "\n",
        "df_sample = pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "kFetOPIsZt74"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview and inspect dataset\n",
        "df_sample.head()"
      ],
      "metadata": {
        "id": "RuFOBJXWXqAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset dimensions and schema\n",
        "\n",
        "print(\"Shape:\", df_sample.shape)\n",
        "print(\"Columns:\", df_sample.columns)\n",
        "df_sample.info()\n",
        "\n"
      ],
      "metadata": {
        "id": "Nugdja8GYdrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Construction using only the columns required for analysis.\n",
        "# Useful columns are: review_id, business_id, stars, text, date\n",
        "#\n",
        "# NOTE: Tried using CSV for the slimmed dataset, however it is still too large to load into Colabs memory.\n",
        "# After research, parquet is the better file format, especially for large datasets for ML/Analytics pipelines\n",
        "\n",
        "# Declare path where reduced dataset files will be written\n",
        "OUT_DIR = (\n",
        "    '/content/drive/Shared drives/Team1Share/GroupProject/'\n",
        "    'Yelp_Dataset/reviews_slim_parquet'\n",
        ")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Record start time to measure end-to-end write performance\n",
        "start_time = time.time()\n",
        "\n",
        "# Counter used to name sequential Parquet part files\n",
        "n_parts = 0\n",
        "\n",
        "# Stream the large JSON Lines file in blocks to avoid memory issues\n",
        "# Each iteration writes one self-contained Parquet partition;\n",
        "# the full dataset is represented by the collection of part files\n",
        "for df_block in pd.read_json(DATA_PATH, lines=True, chunksize=100_000):\n",
        "    df_block = df_block[['review_id', 'business_id', 'stars', 'text', 'date']]\n",
        "\n",
        "    # Construct a unique filename for this block to create a partitioned dataset\n",
        "    part_path = os.path.join(OUT_DIR, f'part_{n_parts:05d}.parquet')\n",
        "    # Write the current block to disk as a compressed Parquet file\n",
        "    df_block.to_parquet(part_path, compression='snappy', index=False)\n",
        "\n",
        "    # Increment part counter for the next block\n",
        "    n_parts += 1\n",
        "\n",
        "# Convert elapsed time into minutes and seconds for readability\n",
        "elapsed_time = time.time() - start_time\n",
        "minutes = int(elapsed_time // 60)\n",
        "seconds = elapsed_time % 60\n",
        "\n",
        "# Report output location, number of files written, and runtime\n",
        "print(f\"Reduced dataset written to directory: {OUT_DIR}\")\n",
        "print(f\"Parquet files written: {n_parts}\")\n",
        "print(f\"Time elapsed: {minutes} minutes {seconds:.1f} seconds\")\n"
      ],
      "metadata": {
        "id": "B_7RwkAIPpXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load reduced dataset from Parquet directory\n",
        "# This cell is expensive, run it by itself\n",
        "\n",
        "df_reduced = pd.read_parquet(OUT_DIR)\n",
        "\n"
      ],
      "metadata": {
        "id": "-AMeDXrRpZA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview Data\n",
        "\n",
        "df_reduced.head()\n"
      ],
      "metadata": {
        "id": "xyv2EgsWqA6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine dataset structure\n",
        "\n",
        "df_reduced.shape\n",
        "df_reduced.info()"
      ],
      "metadata": {
        "id": "7kGa9CpwqXh_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}